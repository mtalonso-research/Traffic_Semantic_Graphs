{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a46fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5259c",
   "metadata": {},
   "source": [
    "## Phase 1: Generating the Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e440404",
   "metadata": {},
   "source": [
    "#### 1. Download Data\n",
    "\n",
    "- To download only one episode, use data_downloader(ep_num,...)\n",
    "\n",
    "- To download a range of episodes, use data_downloader(min_ep,max_ep,...)\n",
    "\n",
    "- To download a list of episodes, use data_downloader(list,...)\n",
    "\n",
    "- Additionally, you can chnge the following directories:\n",
    "\n",
    "    - tabular_data_dir = '../data/raw/L2D/tabular'\n",
    "\n",
    "    - frames_dir = '../data/raw/L2D/frames'\n",
    "\n",
    "NOTE: All cameras are currently set to false until we figure out how to match camera data to tabular data in the new release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdab3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.load_data_L2D import data_downloader\n",
    "\n",
    "data_downloader(0,n_secs=3,\n",
    "                features={\"tabular\": True,\n",
    "                          \"frames\": {\n",
    "                                'observation.images.front_left': False,\n",
    "                                'observation.images.left_backward': False,\n",
    "                                'observation.images.left_forward': False,\n",
    "                                'observation.images.map': False,\n",
    "                                'observation.images.rear': False,\n",
    "                                'observation.images.right_backward': False,\n",
    "                                'observation.images.right_forward': False,\n",
    "                            }\n",
    "                        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44174f1",
   "metadata": {},
   "source": [
    "#### 2. Process Tabular Data & Add Tags\n",
    "\n",
    "- To process only one episode, use process_tabular_data(ep_num,...)\n",
    "\n",
    "- To process a range of episodes, use process_tabular_data(min_ep,max_ep,...)\n",
    "\n",
    "- To process a list of episodes, use process_tabular_data(list,...)\n",
    "\n",
    "- Additionally, you can change the following directories:\n",
    "\n",
    "    - process_tabular_data:\n",
    "\n",
    "        - source_dir = '../data/raw/L2D/tabular'\n",
    "\n",
    "        - output_dir_processed = '../data/processed_data/L2D'\n",
    "\n",
    "        - output_dir_tags = '../data/semantic_tags/L2D'\n",
    "\n",
    "    - add_data_tags:\n",
    "\n",
    "        - data_dir = '../data/processed/L2D'\n",
    "\n",
    "        - tags_dir='../data/semantic_tags/L2D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.process_tabular_data_L2D import process_tabular_split\n",
    "\n",
    "process_tabular_split(0,\n",
    "                    source_dir='../data/raw/L2D/tabular',\n",
    "                    output_dir='../data/split/L2D/tabular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.process_tabular_data_L2D import process_tabular_data\n",
    "from functions.process_tags_L2D import add_data_tags\n",
    "\n",
    "process_tabular_data(0,\n",
    "                    overwrite=True, process_columns=True, \n",
    "                    process_osm=False, process_turning=True,\n",
    "                    time_sleep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.process_tags_L2D import add_data_tags\n",
    "\n",
    "add_data_tags(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b47a1",
   "metadata": {},
   "source": [
    "#### 3. Process Frames\n",
    "\n",
    "- Set up with depth model with pip install git+https://github.com/apple/ml-depth-pro.git\n",
    "\n",
    "- Set up rfdetr with pip install --ignore-installed rfdetr\n",
    "\n",
    "- To process only one episode, use process_frames(ep_num,...)\n",
    "\n",
    "- To process a range of episodes, use process_frames(min_ep,max_ep,...)\n",
    "\n",
    "- To process a list of episodes, use process_frames(list,...)\n",
    "\n",
    "- Additionally, you can change the following directories:\n",
    "\n",
    "    - input_base_dir = '../data/raw/L2D/frames',\n",
    "    \n",
    "    - output_base_dir = '../data/processed_frames/L2D'\n",
    "\n",
    "- You can also include the following cameras:\n",
    "\n",
    "    - \"observation.images.front_left\",\n",
    "    - \"observation.images.left_forward\", \n",
    "    - \"observation.images.right_forward\",\n",
    "    - \"observation.images.right_backward\",\n",
    "    - \"observation.images.rear\",\n",
    "    - \"observation.images.left_backward\"\n",
    "\n",
    "**Note**: Including additional cameras will help with the depth, speed, etc. but CONSIDERABLY increases running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d70dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.process_frames_L2D import process_frames\n",
    "\n",
    "process_frames(0,\n",
    "               cameras_on=[\"observation.images.front_left\"],\n",
    "               run_dict={\"detection\": True,\n",
    "                         \"depth\": True,\n",
    "                         \"speed\": True,\n",
    "                         'overwrite': True},\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.process_lanes_L2D import lane_processing\n",
    "summary = lane_processing(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f35ae0",
   "metadata": {},
   "source": [
    "#### 4. Generate Graphs (Need to Update for the Addition of Speed)\n",
    "\n",
    "- To process only one episode, use generate_graphs(ep_num,...)\n",
    "\n",
    "- To process a range of episodes, use generate_graphs(min_ep,max_ep,...)\n",
    "\n",
    "- To process a list of episodes, use generate_graphs(list,...)\n",
    "\n",
    "- Additionally, you can change the following directories:\n",
    "\n",
    "    - source_data_dir = '../data/processed/L2D',\n",
    "    \n",
    "    - processed_frame_dir = '../data/processed_frames/L2D',\n",
    "                    \n",
    "    - output_dir = '../data/graphical/L2D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.graphs import generate_graphs\n",
    "generate_graphs(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f9d08",
   "metadata": {},
   "source": [
    "#### Quick Visualization of Graph & Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d9cacd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a90d89aaa0b422d818ae96748551339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CytoscapeWidget(cytoscape_layout={'name': 'cola'}, cytoscape_style=[{'selector': 'node', 'style': {'label': 'dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functions.graphs import combined_graph_viewer\n",
    "\n",
    "ep_num = 0\n",
    "with open(f\"../data/graphical/l2d/{ep_num}_graph.json\", \"r\") as f:\n",
    "    graph_data = json.load(f)\n",
    "\n",
    "cyto = combined_graph_viewer({'graph':graph_data})\n",
    "display(cyto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e21adf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy': 'STUDENT',\n",
       " 'instructions': 'Drive straight up to the yield sign and take first exit from the roundabout',\n",
       " 'action_tag': 'roundabout',\n",
       " 'traffic_control_tag': 'roundabout',\n",
       " 'road_feature_tags': ['bike_friendly',\n",
       "  'pedestrian_area',\n",
       "  'rural',\n",
       "  'sidewalk_present'],\n",
       " 'environment_tags': ['low_visibility_possible',\n",
       "  'off_peak_hours',\n",
       "  'winter_conditions_possible']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_num = 0\n",
    "with open(f\"../data/semantic_tags/l2d/{ep_num}_graph.json\", \"r\") as f:\n",
    "    graph_tags = json.load(f)\n",
    "graph_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb2db78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                            Veh_FL_A_1\n",
       "type                                                             vehicle\n",
       "lane_classification                                     out_of_lane_left\n",
       "lane_overlap_ratio                                              0.003677\n",
       "distance_m                                                      3.432257\n",
       "dist_to_ego                                                     4.675622\n",
       "speed_ms                                                        9.176929\n",
       "speed_kmh                                                      33.036946\n",
       "velocity_ms            [-8.72930463576159, -2.8311258278145695, -4.52...\n",
       "velocity_kmh           [-31.425496688741724, -10.19205298013245, -16....\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functions.graphs import graph_to_dfs\n",
    "\n",
    "nodes_df, edges_df = graph_to_dfs(graph_data)\n",
    "nodes_df.iloc[8].dropna()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem_graphs (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
