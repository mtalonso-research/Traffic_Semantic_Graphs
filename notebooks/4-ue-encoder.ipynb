{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3f2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader \n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06ca9e",
   "metadata": {},
   "source": [
    "#### 1. Make Data Loaders\n",
    "\n",
    "- Please indicate the directory where the json graphical dataset is located\n",
    "\n",
    "- Additionally you can change the following parameters:\n",
    "\n",
    "    - mode: This lets you choose which features to include in the loaded data. The options are:\n",
    "        - all: includes all nodes and edges\n",
    "        - ego_veh: includes all ego and vehicle nodes and edges\n",
    "        - ego: includes only ego nodes and edges\n",
    "        - ego_env: includes all ego and environment nodes and edges\n",
    "        - all_no_edges: includes all node features but no edges\n",
    "        - ego_veh_no_edges: includes all ego and vehicle node features but no edges\n",
    "        - ego_no_edges: includes only ego node features with no edges\n",
    "        - ego_env_no_edges: includes all ego and environment node features but no edges\n",
    "\n",
    "    - normalize: True/False. If true, this applies a normalization transform + nan to num\n",
    "\n",
    "    - norm_methos: zscore or l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c287c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total graphs: 9593\n",
      "HeteroDataBatch(\n",
      "  ego={\n",
      "    x=[492, 6],\n",
      "    batch=[492],\n",
      "    ptr=[65],\n",
      "  },\n",
      "  window_meta={ episode_path=[64] },\n",
      "  (ego, to, ego)={ edge_index=[2, 428] }\n",
      ")\n",
      "===========\n",
      "Total graphs: 88370\n",
      "HeteroDataBatch(\n",
      "  ego={\n",
      "    x=[517, 6],\n",
      "    batch=[517],\n",
      "    ptr=[65],\n",
      "  },\n",
      "  window_meta={ episode_path=[64] },\n",
      "  (ego, to, ego)={ edge_index=[2, 453] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from functions.data_loaders import get_graph_dataset\n",
    "\n",
    "l2d_train_set = get_graph_dataset(\"../data/processed_graphical/l2d/\", \n",
    "                                  mode=\"ego\", normalize=True, norm_method=\"zscore\")\n",
    "l2d_train_loader = DataLoader(l2d_train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Total graphs: {len(l2d_train_set)}\")  \n",
    "for data in l2d_train_loader:\n",
    "    print(data) \n",
    "    break\n",
    "\n",
    "print('===========')\n",
    "\n",
    "nup_train_set = get_graph_dataset(\"../data/processed_graphical/nuplan/\", \n",
    "                                  mode=\"ego\", normalize=True, norm_method=\"zscore\")\n",
    "nup_train_loader = DataLoader(l2d_train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Total graphs: {len(nup_train_set)}\")  \n",
    "for data in l2d_train_loader:\n",
    "    print(data) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735fe2a",
   "metadata": {},
   "source": [
    "### ðŸš§ Under Construction ðŸš§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ceed112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2D node dims: {'ego': 6}\n",
      "NUP node dims: {'ego': 6}\n"
     ]
    }
   ],
   "source": [
    "from functions.models import GraphEmbedder, ProjectionHead, kl_divergence_between_gaussians\n",
    "\n",
    "batch_l2d = next(iter(l2d_train_loader))\n",
    "batch_nup = next(iter(nup_train_loader))\n",
    "\n",
    "node_dims_l2d = {\n",
    "    node_type: batch_l2d[node_type].x.size(1)\n",
    "    for node_type in batch_l2d.node_types\n",
    "    if hasattr(batch_l2d[node_type], 'x')\n",
    "}\n",
    "\n",
    "node_dims_nup = {\n",
    "    node_type: batch_nup[node_type].x.size(1)\n",
    "    for node_type in batch_nup.node_types\n",
    "    if hasattr(batch_nup[node_type], 'x')\n",
    "}\n",
    "\n",
    "print(\"L2D node dims:\", node_dims_l2d)\n",
    "print(\"NUP node dims:\", node_dims_nup)\n",
    "\n",
    "l2d_encoder = GraphEmbedder(node_dims_l2d, hidden_dims=[64,128,32])\n",
    "nup_encoder = GraphEmbedder(node_dims_nup, hidden_dims=[64,128,32])\n",
    "projector = ProjectionHead(in_dim=32, proj_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76393f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3428.4546, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_l2d = projector(l2d_encoder(batch_l2d))  # [B, 128]\n",
    "z_nup = projector(nup_encoder(batch_nup))  # [B, 128]\n",
    "loss = kl_divergence_between_gaussians(z_l2d, z_nup)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481565d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Send models to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "l2d_encoder = l2d_encoder.to(device)\n",
    "nup_encoder = nup_encoder.to(device)\n",
    "projector = projector.to(device)\n",
    "\n",
    "# Optimizer for all params\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(l2d_encoder.parameters()) + \n",
    "    list(nup_encoder.parameters()) + \n",
    "    list(projector.parameters()), \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l2d_encoder.train()\n",
    "    nup_encoder.train()\n",
    "    projector.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(zip(l2d_train_loader, nup_train_loader), \n",
    "                total=min(len(l2d_train_loader), len(nup_train_loader)), \n",
    "                desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch_l2d, batch_nup in pbar:\n",
    "        batch_l2d = batch_l2d.to(device)\n",
    "        batch_nup = batch_nup.to(device)\n",
    "\n",
    "        z_l2d = projector(l2d_encoder(batch_l2d))  # [B, D]\n",
    "        z_nup = projector(nup_encoder(batch_nup))  # [B, D]\n",
    "\n",
    "        z_l2d = F.normalize(z_l2d, dim=-1)\n",
    "        z_nup = F.normalize(z_nup, dim=-1)\n",
    "        loss = kl_divergence_between_gaussians(z_l2d, z_nup) + kl_divergence_between_gaussians(z_nup, z_l2d)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(pbar)\n",
    "    print(f\"Epoch {epoch+1} complete. Average KL Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
